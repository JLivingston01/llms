{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Example text\n",
    "long_text = \"\"\"\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer nec odio. Praesent libero. Sed cursus ante dapibus diam. Sed nisi. Nulla quis sem at nibh elementum imperdiet. Duis sagittis ipsum. Praesent mauris. Fusce nec tellus sed augue semper porta. Mauris massa. Vestibulum lacinia arcu eget nulla. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. \n",
    "\n",
    "In hac habitasse platea dictumst. Curabitur sodales ligula in libero. Sed dignissim lacinia nunc. Curabitur tortor. Pellentesque nibh. Aenean quam. In scelerisque sem at dolor. Maecenas mattis. Sed convallis tristique sem. Proin ut ligula vel nunc egestas porttitor. Morbi lectus risus, iaculis vel, suscipit quis, luctus non, massa. Fusce ac turpis quis ligula lacinia aliquet. Mauris ipsum. Nulla metus metus, ullamcorper vel, tincidunt sed, euismod in, nibh. Quisque volutpat condimentum velit. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. \n",
    "\n",
    "Nam nec ante. Sed lacinia, urna non tincidunt mattis, tortor neque adipiscing diam, a cursus ipsum ante quis turpis. Nulla facilisi. Ut fringilla. Suspendisse potenti. Nunc feugiat mi a tellus consequat imperdiet. Vestibulum sapien. Proin quam. Etiam ultrices. Suspendisse in justo eu magna luctus suscipit. Sed lectus. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_text = tokenizer.encode(long_text, add_special_tokens=True, max_length=512, truncation=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\n",
    "First line. \n",
    "\n",
    "second line\n",
    "\n",
    "third line!\n",
    "\"\"\"\n",
    "\n",
    "enc = tokenizer.encode(txt, add_special_tokens=True, max_length=512, truncation=True)\n",
    "\n",
    "enc\n",
    "\n",
    "dec = tokenizer.decode(enc)\n",
    "\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtlst = [\n",
    "\"\"\"first line. \n",
    "\n",
    "second line\n",
    "\n",
    "third Line!\"\"\",\n",
    "\"new document\",\n",
    "\"new single line\"\n",
    "]\n",
    "\n",
    "# Tokenize each document separately and concatenate them with separator tokens\n",
    "tokenized_documents = []\n",
    "for doc in txtlst:\n",
    "    tokenized_doc = tokenizer.encode(doc, add_special_tokens=False)\n",
    "    tokenized_documents.extend(tokenized_doc + [tokenizer.eos_token_id])  # Adding end-of-sequence token between documents\n",
    "\n",
    "\n",
    "tokenized_documents\n",
    "\n",
    "dec = [tokenizer.decode(enc) for enc in tokenized_documents]\n",
    "\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOTU\n",
    "\n",
    "from nltk.corpus import state_union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = [state_union.raw(f) for f in state_union.fileids()]\n",
    "\n",
    "tokenized_docs = []\n",
    "for doc in docs:\n",
    "    tokenized_doc = tokenizer.encode(doc, add_special_tokens=False, \n",
    "                                     #max_length=1600, \n",
    "                                     #truncation=False\n",
    "                                     )\n",
    "    tokenized_docs.extend(tokenized_doc + [tokenizer.eos_token_id])  # Adding end-of-sequence token between documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs[:100]\n",
    "\n",
    "dec = tokenizer.decode(tokenized_docs[:100])\n",
    "\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device='cpu'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "\n",
    "#import numpy as np\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_sequence_length=5000,device='cpu'):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        position = torch.arange(0, max_sequence_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2) * -(math.log(10000.0) / embed_size))\n",
    "        self.positional_encoding = torch.zeros(max_sequence_length, embed_size)\n",
    "        self.positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.positional_encoding = self.positional_encoding.unsqueeze(0).to(device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.positional_encoding[:, :x.size(1)].detach()\n",
    "\n",
    "\n",
    "# Define your dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "# Define the Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, hidden_size, num_layers, dropout_prob, max_sequence_length=5000,device='cpu'):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = PositionalEncoding(embed_size, max_sequence_length,device=device)\n",
    "        self.transformer_encoder_layers = TransformerEncoderLayer(embed_size, num_heads, hidden_size, dropout_prob)\n",
    "        self.transformer_encoder = TransformerEncoder(self.transformer_encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded_with_position = self.positional_encoding(embedded)\n",
    "        embedded_with_position = embedded_with_position.permute(1, 0, 2)  # Transformer expects sequence length first\n",
    "        output = self.transformer_encoder(embedded_with_position)\n",
    "        output = output.permute(1, 0, 2)  # Back to batch first\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "    \n",
    "# Define a function to generate text given a starting prompt\n",
    "def generate_text(model, prompt, max_length=100, temperature=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        current_token = torch.tensor([[prompt]], dtype=torch.long)\n",
    "        output_sequence = [prompt]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            logits = model(current_token)\n",
    "            logits = logits[0, -1, :] / temperature\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, 1)\n",
    "            output_sequence.append(next_token.item())\n",
    "            current_token = next_token.unsqueeze(0)\n",
    "            if next_token == 1:  # Stop generating if EOS token is generated\n",
    "                break\n",
    "                \n",
    "    return output_sequence\n",
    "\n",
    "def generate_text_cuda(model, prompt, max_length=100, temperature=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        current_token = torch.tensor([[prompt]], dtype=torch.long).cuda()\n",
    "        output_sequence = [prompt]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            logits = model(current_token)\n",
    "            logits = logits[0, -1, :] / temperature\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, 1)\n",
    "            output_sequence.append(next_token.item())\n",
    "            current_token = next_token.unsqueeze(0)\n",
    "            if next_token == 1:  # Stop generating if EOS token is generated\n",
    "                break\n",
    "                \n",
    "    return output_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 9.345067977905273\n",
      "Epoch [2/10], Loss: 9.10288143157959\n",
      "Epoch [3/10], Loss: 8.877528190612793\n",
      "Epoch [4/10], Loss: 8.660417556762695\n",
      "Epoch [5/10], Loss: 8.456753730773926\n",
      "Epoch [6/10], Loss: 8.261812210083008\n",
      "Epoch [7/10], Loss: 8.048837661743164\n",
      "Epoch [8/10], Loss: 7.841767311096191\n",
      "Epoch [9/10], Loss: 7.635434627532959\n",
      "Epoch [10/10], Loss: 7.436553001403809\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Dummy data for demonstration\n",
    "seq_length = 20\n",
    "batch_size = 32\n",
    "vocab_size = 10000\n",
    "dummy_input = torch.randint(0, vocab_size, (seq_length, batch_size))  # Random input sequence\n",
    "\n",
    "# Hyperparameters\n",
    "embed_size = 128\n",
    "num_heads = 4\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "dropout_prob = 0.1\n",
    "\n",
    "# Instantiate the model\n",
    "model = TransformerModel(vocab_size, embed_size, num_heads, hidden_size, num_layers, dropout_prob)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop (dummy example)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(dummy_input)\n",
    "    loss = criterion(logits.view(-1, vocab_size), dummy_input.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Save the trained model\n",
    "#torch.save(model.state_dict(), 'small_transformer_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence: [10, 9906, 4485, 3379, 2575, 4871, 1690, 8158, 529, 3708, 8759, 2922, 9740, 1695, 1498, 7538, 2346, 9762, 7206, 4156, 901, 5765, 7035, 8598, 7048, 9014, 7972, 9525, 5764, 6930, 508, 8478, 9505, 2680, 540, 2939, 4200, 3585, 5569, 8594, 6750, 3076, 5262, 6919, 3901, 2126, 4422, 1098, 6945, 7915, 3519, 928, 6261, 671, 188, 3948, 5523, 5015, 8954, 556, 495, 9509, 2258, 5571, 2533, 6244, 3202, 1785, 5253, 4033, 8984, 8994, 8870, 6128, 1999, 1763, 5037, 5721, 4980, 4682, 9374, 6571, 4989, 7951, 589, 2674, 1797, 5766, 3641, 563, 3925, 2646, 7261, 6291, 6276, 2901, 865, 9560, 8785, 3246, 6569]\n"
     ]
    }
   ],
   "source": [
    "generated_sequence = generate_text(model, prompt=10)  # You can provide any integer as the starting prompt\n",
    "print('Generated sequence:', generated_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2264 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import state_union\n",
    "\n",
    "docs = [state_union.raw(f) for f in state_union.fileids()]\n",
    "\n",
    "tokenized_docs = []\n",
    "for doc in docs:\n",
    "    tokenized_doc = tokenizer.encode(doc, add_special_tokens=False, \n",
    "                                     #max_length=1600, \n",
    "                                     #truncation=False\n",
    "                                     )\n",
    "    tokenized_docs.extend(tokenized_doc + [tokenizer.eos_token_id])  # Adding end-of-sequence token between documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(tokenized_docs)\n",
    "\n",
    "dataset = MyDataset(input_ids)\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Hyperparameters\n",
    "num_heads = 6\n",
    "embed_subsize = 32\n",
    "embed_size = num_heads*embed_subsize\n",
    "hidden_size = num_heads*embed_subsize\n",
    "num_layers = 4\n",
    "dropout_prob = 0.1\n",
    "\n",
    "\n",
    "model = TransformerModel(vocab_size, embed_size, num_heads, hidden_size, num_layers, dropout_prob,device='cuda').cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Loss: 4087.267250061035\n",
      "Epoch [2/2], Loss: 1798.0610418319702\n"
     ]
    }
   ],
   "source": [
    "# Training loop (dummy example)\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in dataloader:\n",
    "\n",
    "        batch=batch.cuda().unsqueeze(0)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch)\n",
    "        loss = criterion(logits.view(-1, vocab_size), batch.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss+=loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence: [16000, 3336, 8741, 17984, 23637, 7876, 523, 662, 8904, 13293, 2372, 12859, 18424, 2793, 7584, 4385, 6303, 16826, 19586, 20903, 5091, 12546, 44129, 1365, 14923, 4441, 257, 45338, 5716, 26718, 2756, 1363, 17413, 32288, 16247, 3394, 3812, 76, 5975, 4133, 43270, 18147, 11798, 503, 8598, 39246, 29874, 263, 15055, 14186, 3395, 32293, 10219, 6961, 7763, 39530, 2222, 38032, 6875, 18385, 1381, 22459, 1957, 1904, 8494, 27522, 15331, 4290, 11501, 1367, 3379, 47649, 30383, 1415, 20197, 9, 326, 3452, 1716, 4906, 869, 3265, 41495, 7918, 21487, 9194, 1957, 6556, 12356, 7810, 1069, 13, 554, 22584, 780, 1219, 14607, 23781, 502, 2236, 1365]\n"
     ]
    }
   ],
   "source": [
    "generated_sequence = generate_text_cuda(model, prompt=16000,temperature=1)  # You can provide any integer as the starting prompt\n",
    "print('Generated sequence:', generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Message THE Nation observers dialect urban so pre substantial achievement threat�icable lower puts supposed 1990 willingness Marines finances occurred disagree Kodi better rumors creating a kins treated segregation price home steppingregor dignity Russian towardm surprise resourcesrocal Cuban industries out mountain DeskArthurer triple precious Met benign initiative proposal contained Dyn bringperia declared tuitionats preservation localuse solve Thought monetary citizens wisdom 11iableAuthentITIES14 Revenue* that latest becometype call populationmeat myth utilized producing local motiv talented colleaguesex. Infact becauseoh reasoning um me shall better'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.decode(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 20464465\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynvml\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_gpu_temperature():\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # Assuming you have only one GPU\n",
    "    gpu_temperature = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
    "    pynvml.nvmlShutdown()\n",
    "    return gpu_temperature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Temperature: 70 degrees Celsius\n"
     ]
    }
   ],
   "source": [
    "\n",
    "temperature = get_gpu_temperature()\n",
    "print(f\"GPU Temperature: {temperature} degrees Celsius\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 703.6633914113045\n",
      "Epoch [2/30], Loss: 177.0269646421075\n",
      "Sleeping...75\n",
      "Epoch [3/30], Loss: 49.81680866423994\n",
      "Epoch [4/30], Loss: 9.126902840391267\n",
      "Sleeping...75\n",
      "Epoch [5/30], Loss: 1.4451938063430134\n",
      "Epoch [6/30], Loss: 0.7366683402215131\n",
      "Sleeping...76\n",
      "Epoch [7/30], Loss: 0.43585958385665435\n",
      "Epoch [8/30], Loss: 0.2698445964997518\n",
      "Sleeping...75\n",
      "Epoch [9/30], Loss: 0.17099054516802425\n",
      "Epoch [10/30], Loss: 0.10983816627049237\n",
      "Sleeping...76\n",
      "Epoch [11/30], Loss: 0.07112167431114358\n",
      "Epoch [12/30], Loss: 0.04627244730909297\n",
      "Sleeping...75\n",
      "Epoch [13/30], Loss: 0.030197119576769182\n",
      "Epoch [14/30], Loss: 0.019742071815471718\n",
      "Sleeping...75\n",
      "Epoch [15/30], Loss: 0.012926153644002625\n",
      "Epoch [16/30], Loss: 0.008474889194530988\n",
      "Sleeping...76\n",
      "Epoch [17/30], Loss: 0.0055655126657256915\n",
      "Epoch [18/30], Loss: 0.0036631661948831606\n",
      "Epoch [19/30], Loss: 0.002419059901399123\n",
      "Sleeping...77\n",
      "Epoch [20/30], Loss: 0.001604369345784562\n",
      "Epoch [21/30], Loss: 0.0010699926142478944\n",
      "Sleeping...75\n",
      "Epoch [22/30], Loss: 0.0007197073065867698\n",
      "Epoch [23/30], Loss: 0.0004908364261524412\n",
      "Sleeping...75\n",
      "Epoch [24/30], Loss: 0.0003408021827766561\n",
      "Epoch [25/30], Loss: 0.00024281324297703577\n",
      "Sleeping...76\n",
      "Epoch [26/30], Loss: 0.00017744781281692212\n",
      "Epoch [27/30], Loss: 0.00013295675098135007\n",
      "Sleeping...75\n",
      "Epoch [28/30], Loss: 1.2694602973140299\n",
      "Epoch [29/30], Loss: 0.4066893574402002\n",
      "Sleeping...75\n",
      "Epoch [30/30], Loss: 0.005352467896500457\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch = 0\n",
    "num_epochs = 30\n",
    "\n",
    "while epoch < num_epochs:\n",
    "\n",
    "    temperature = get_gpu_temperature()\n",
    "\n",
    "    if temperature < 75:\n",
    "\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in dataloader:\n",
    "\n",
    "            batch=batch.cuda().unsqueeze(0)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch)\n",
    "            loss = criterion(logits.view(-1, vocab_size), batch.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss+=loss.item()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss}')\n",
    "        epoch+=1\n",
    "    \n",
    "    else:\n",
    "        print(f\"Sleeping...{temperature}\")\n",
    "        time.sleep(30)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence: [16000, 39981, 7622, 33019, 9839, 17093, 384, 19773, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262]\n"
     ]
    }
   ],
   "source": [
    "generated_sequence = generate_text_cuda(model, prompt=16000,temperature=1)  # You can provide any integer as the starting prompt\n",
    "print('Generated sequence:', generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Message assassinated keeps slows tie unrest se Mohammed the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
