{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Example text\n",
    "long_text = \"\"\"\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer nec odio. Praesent libero. Sed cursus ante dapibus diam. Sed nisi. Nulla quis sem at nibh elementum imperdiet. Duis sagittis ipsum. Praesent mauris. Fusce nec tellus sed augue semper porta. Mauris massa. Vestibulum lacinia arcu eget nulla. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. \n",
    "\n",
    "In hac habitasse platea dictumst. Curabitur sodales ligula in libero. Sed dignissim lacinia nunc. Curabitur tortor. Pellentesque nibh. Aenean quam. In scelerisque sem at dolor. Maecenas mattis. Sed convallis tristique sem. Proin ut ligula vel nunc egestas porttitor. Morbi lectus risus, iaculis vel, suscipit quis, luctus non, massa. Fusce ac turpis quis ligula lacinia aliquet. Mauris ipsum. Nulla metus metus, ullamcorper vel, tincidunt sed, euismod in, nibh. Quisque volutpat condimentum velit. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. \n",
    "\n",
    "Nam nec ante. Sed lacinia, urna non tincidunt mattis, tortor neque adipiscing diam, a cursus ipsum ante quis turpis. Nulla facilisi. Ut fringilla. Suspendisse potenti. Nunc feugiat mi a tellus consequat imperdiet. Vestibulum sapien. Proin quam. Etiam ultrices. Suspendisse in justo eu magna luctus suscipit. Sed lectus. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_text = tokenizer.encode(long_text, add_special_tokens=True, max_length=512, truncation=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\n",
    "First line. \n",
    "\n",
    "second line\n",
    "\n",
    "third line!\n",
    "\"\"\"\n",
    "\n",
    "enc = tokenizer.encode(txt, add_special_tokens=True, max_length=512, truncation=True)\n",
    "\n",
    "enc\n",
    "\n",
    "dec = tokenizer.decode(enc)\n",
    "\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtlst = [\n",
    "\"\"\"first line. \n",
    "\n",
    "second line\n",
    "\n",
    "third Line!\"\"\",\n",
    "\"new document\",\n",
    "\"new single line\"\n",
    "]\n",
    "\n",
    "# Tokenize each document separately and concatenate them with separator tokens\n",
    "tokenized_documents = []\n",
    "for doc in txtlst:\n",
    "    tokenized_doc = tokenizer.encode(doc, add_special_tokens=False)\n",
    "    tokenized_documents.extend(tokenized_doc + [tokenizer.eos_token_id])  # Adding end-of-sequence token between documents\n",
    "\n",
    "\n",
    "tokenized_documents\n",
    "\n",
    "dec = [tokenizer.decode(enc) for enc in tokenized_documents]\n",
    "\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOTU\n",
    "\n",
    "from nltk.corpus import state_union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = [state_union.raw(f) for f in state_union.fileids()]\n",
    "\n",
    "tokenized_docs = []\n",
    "for doc in docs:\n",
    "    tokenized_doc = tokenizer.encode(doc, add_special_tokens=False, \n",
    "                                     #max_length=1600, \n",
    "                                     #truncation=False\n",
    "                                     )\n",
    "    tokenized_docs.extend(tokenized_doc + [tokenizer.eos_token_id])  # Adding end-of-sequence token between documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs[:100]\n",
    "\n",
    "dec = tokenizer.decode(tokenized_docs[:100])\n",
    "\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c8c4d6dcae43dc8cb10b859fb86712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jason\\Desktop\\git_control\\llms\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Jason\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebadc3a9f9e9464a9059c95b73a7f20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ea00c64406435fb3611eeae5b6c52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bb0ea07b864e1685cfc80060af36ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#import numpy as np\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "# Define your dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "# Define the Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, hidden_size, num_layers, dropout_prob):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.transformer_encoder_layers = TransformerEncoderLayer(embed_size, num_heads, hidden_size, dropout_prob)\n",
    "        self.transformer_encoder = TransformerEncoder(self.transformer_encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = embedded.permute(1, 0, 2)  # Transformer expects sequence length first\n",
    "        output = self.transformer_encoder(embedded)\n",
    "        output = output.permute(1, 0, 2)  # Back to batch first\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "    \n",
    "# Define a function to generate text given a starting prompt\n",
    "def generate_text(model, prompt, max_length=100, temperature=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        current_token = torch.tensor([[prompt]], dtype=torch.long)\n",
    "        output_sequence = [prompt]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            logits = model(current_token)\n",
    "            logits = logits[0, -1, :] / temperature\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, 1)\n",
    "            output_sequence.append(next_token.item())\n",
    "            current_token = next_token.unsqueeze(0)\n",
    "            if next_token == 1:  # Stop generating if EOS token is generated\n",
    "                break\n",
    "                \n",
    "    return output_sequence\n",
    "\n",
    "def generate_text_cuda(model, prompt, max_length=100, temperature=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        current_token = torch.tensor([[prompt]], dtype=torch.long).cuda()\n",
    "        output_sequence = [prompt]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            logits = model(current_token)\n",
    "            logits = logits[0, -1, :] / temperature\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, 1)\n",
    "            output_sequence.append(next_token.item())\n",
    "            current_token = next_token.unsqueeze(0)\n",
    "            if next_token == 1:  # Stop generating if EOS token is generated\n",
    "                break\n",
    "                \n",
    "    return output_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 9.337352752685547\n",
      "Epoch [2/10], Loss: 9.124256134033203\n",
      "Epoch [3/10], Loss: 8.911956787109375\n",
      "Epoch [4/10], Loss: 8.702798843383789\n",
      "Epoch [5/10], Loss: 8.495234489440918\n",
      "Epoch [6/10], Loss: 8.279451370239258\n",
      "Epoch [7/10], Loss: 8.070791244506836\n",
      "Epoch [8/10], Loss: 7.855935573577881\n",
      "Epoch [9/10], Loss: 7.648670196533203\n",
      "Epoch [10/10], Loss: 7.4374823570251465\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Dummy data for demonstration\n",
    "seq_length = 20\n",
    "batch_size = 32\n",
    "vocab_size = 10000\n",
    "dummy_input = torch.randint(0, vocab_size, (seq_length, batch_size))  # Random input sequence\n",
    "\n",
    "# Hyperparameters\n",
    "embed_size = 128\n",
    "num_heads = 4\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "dropout_prob = 0.1\n",
    "\n",
    "# Instantiate the model\n",
    "model = TransformerModel(vocab_size, embed_size, num_heads, hidden_size, num_layers, dropout_prob)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop (dummy example)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(dummy_input)\n",
    "    loss = criterion(logits.view(-1, vocab_size), dummy_input.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Save the trained model\n",
    "#torch.save(model.state_dict(), 'small_transformer_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence: [10, 2113, 2769, 3475, 6870, 4146, 1433, 8124, 1137, 6457, 6977, 4614, 113, 5238, 8457, 2596, 9320, 6566, 9481, 8181, 3738, 6848, 3120, 8719, 7896, 8487, 3210, 3692, 6693, 4061, 7374, 1765, 3991, 4094, 8489, 3938, 8033, 2718, 9513, 1168, 3036, 6718, 2074, 8283, 1550, 3787, 3688, 6725, 7719, 4979, 1211, 3354, 7915, 9649, 5164, 855, 2560, 8366, 3708, 5680, 4462, 9911, 8246, 7442, 5421, 3131, 1349, 2756, 3387, 4090, 8386, 3654, 3880, 4500, 2579, 6128, 7480, 4810, 1566, 1895, 6257, 3235, 580, 3350, 3093, 7993, 9545, 9725, 6871, 6639, 288, 2647, 1378, 2923, 1705, 4109, 295, 1354, 2907, 3536, 1058]\n"
     ]
    }
   ],
   "source": [
    "generated_sequence = generate_text(model, prompt=10)  # You can provide any integer as the starting prompt\n",
    "print('Generated sequence:', generated_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2264 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import state_union\n",
    "\n",
    "docs = [state_union.raw(f) for f in state_union.fileids()]\n",
    "\n",
    "tokenized_docs = []\n",
    "for doc in docs:\n",
    "    tokenized_doc = tokenizer.encode(doc, add_special_tokens=False, \n",
    "                                     #max_length=1600, \n",
    "                                     #truncation=False\n",
    "                                     )\n",
    "    tokenized_docs.extend(tokenized_doc + [tokenizer.eos_token_id])  # Adding end-of-sequence token between documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(tokenized_docs)\n",
    "\n",
    "dataset = MyDataset(input_ids)\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Hyperparameters\n",
    "num_heads = 6\n",
    "embed_subsize = 32\n",
    "embed_size = num_heads*embed_subsize\n",
    "hidden_size = num_heads*embed_subsize\n",
    "num_layers = 4\n",
    "dropout_prob = 0.1\n",
    "\n",
    "\n",
    "model = TransformerModel(vocab_size, embed_size, num_heads, hidden_size, num_layers, dropout_prob).cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 52.50906245177612\n",
      "Epoch [2/10], Loss: 36.409168595215306\n",
      "Epoch [3/10], Loss: 24.076013173209503\n",
      "Epoch [4/10], Loss: 14.805838204803877\n",
      "Epoch [5/10], Loss: 8.428250377532095\n",
      "Epoch [6/10], Loss: 4.603353999147657\n",
      "Epoch [7/10], Loss: 2.5612542165326886\n",
      "Epoch [8/10], Loss: 1.5034951445122715\n",
      "Epoch [9/10], Loss: 0.9270753085438628\n",
      "Epoch [10/10], Loss: 0.5903936814065673\n"
     ]
    }
   ],
   "source": [
    "# Training loop (dummy example)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in dataloader:\n",
    "\n",
    "        batch=batch.cuda().unsqueeze(0)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch)\n",
    "        loss = criterion(logits.view(-1, vocab_size), batch.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss+=loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence: [16000, 16000, 16000, 4768, 10027, 28783, 12394, 12394, 12394, 12394, 12394, 10510, 12795, 4849, 31651, 48517, 21345, 40862, 37440, 5331, 14068, 14494, 2077, 22195, 13637, 2738, 24872, 5327, 18073, 18794, 6114, 226, 3723, 5960, 10946, 30228, 28517, 47886, 45333, 5199, 34130, 19166, 20874, 11684, 20649, 28275, 3849, 7558, 42569, 41381, 8413, 12800, 5611, 11204, 996, 2903, 10342, 15597, 25535, 27270, 10520, 17544, 30904, 4058, 4058, 4058, 4058, 4058, 4058, 4058, 20029, 36835, 24515, 15737, 2676, 16069, 48542, 20712, 26038, 22100, 14271, 14271, 14271, 24597, 1214, 8064, 26063, 11690, 14085, 40134, 3634, 8885, 9123, 11626, 1734, 35010, 6651, 46925, 9759, 20022, 13318]\n"
     ]
    }
   ],
   "source": [
    "generated_sequence = generate_text_cuda(model, prompt=16000,temperature=1)  # You can provide any integer as the starting prompt\n",
    "print('Generated sequence:', generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Message Message Message Louisition heraldINTINTINTINTINT consciousnessclaimed Sal envy democratically Treaty heirsperformingkins treasure lesser taken Senators Rogeritution Chemical clin sponsors dent Victï¿½amingDes theoriesPersonal harmed melancholy marines Robert appalling clips Sugar towns decentral toutedinter constantly sponsoring fadesRem sends launched Being thoughanced searchingKeep thankfulNeitherilation650 inefficient ahead ahead ahead ahead ahead ahead aheadFive epochafort dealersiter rangeswark jungle arbitration fade ammunition ammunition ammunitionBalross ratio 224incoln questioning loafhest enorm scholars tearene simplisticcare deline referring triggers altogether'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.decode(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 20464465\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynvml\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_gpu_temperature():\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # Assuming you have only one GPU\n",
    "    gpu_temperature = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
    "    pynvml.nvmlShutdown()\n",
    "    return gpu_temperature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Temperature: 49 degrees Celsius\n"
     ]
    }
   ],
   "source": [
    "\n",
    "temperature = get_gpu_temperature()\n",
    "print(f\"GPU Temperature: {temperature} degrees Celsius\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Super 1 Epoch [1/10], Loss: 0.0070765627003765985\n",
      "Super 1 Epoch [2/10], Loss: 0.0048304723931096305\n",
      "Super 1 Epoch [3/10], Loss: 0.0033119765279252533\n",
      "Sleeping...75\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch = 0\n",
    "\n",
    "while epoch < num_epochs:\n",
    "\n",
    "    temperature = get_gpu_temperature()\n",
    "\n",
    "    if temperature < 75:\n",
    "\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in dataloader:\n",
    "\n",
    "            batch=batch.cuda().unsqueeze(0)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch)\n",
    "            loss = criterion(logits.view(-1, vocab_size), batch.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss+=loss.item()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss}')\n",
    "        epoch+=1\n",
    "    \n",
    "    else:\n",
    "        print(f\"Sleeping...{temperature}\")\n",
    "        time.sleep(30)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
